\section{Conclusions}

I totally agree on the automation, to ensure objectively and transparency. Its like MSE where you have to agree the OMs and the MP selection criteria in advance.

Also like MSE, I think we need a limited number of complementary but non-redundant metrics. For example are the runs tests,  RMSE and Mohn's rho telling us the same thing? So we are give a single attribute extra weight? Imagine you have a model with a biased index, i.e. a data conflict, that fails the runs and RMSE tests. When you do retrospective you are removing biased points so you will get a large absolute value for Mohn's rho. In contrast MASE for a model free quantity will hopefully be telling you something else. It would be best to recognise that data conflict at the start.

For this reason I  agree with Mark that ideally a model should pass all diagnostics but recognise that this isn't always possible. For example if a model fails to converge that could indicate data conflicts and model misspecification, that will bite you later if not fixed. For example when you start removing points example in the retrospective analysis and hindcasts.

Also while the delta-MVLN approach is useful, I would also like to use estimation error to check bias, i.e. by looking at prediction residuals.

\begin{itemize}
    \item In the stock assessment process hypotheses about states of nature are represented by alternative model structures and fixed parameters. Even with a design of 1440, as in this example, the choice of scenarios is arbitrary and the posterior distributions are often multi-modal.
    
    \item Considering alternative structures and datasets means that it difficult to select models using metrics such as AIC. Therefore retrospective analysis is commonly used to evaluate the stability of stock assessment estimates.  The use of model based quantities, however, means that bias can not be quantified, and may result in future predictions being being poor. We therefore extended retrospective analyses to include predictions. 
   
    \item The use of metrics based on prediction skill allows different data components and model to be compared in order to explore data conflicts and potential model misspecification. The accuracy and precision of predictions depend on the validity of the model, the information in the data, and how far ahead predictions are required. 
   
    \item Retrospective analysis is commonly used to evaluate the stability of stock assessment estimates of model estimates such as stock biomass and exploitation level. Stability is measured using Mohn's $\rho$ a measure of bias. 
    
    \item Shrinking estimates of stock status in the last year to the recent mean can help reduce Mohn's $\rho$. Shrinkage, in statistics, however, is used to reduce mean squared error (MSE), at the expense of bias. 
    
    \item The use of model based quantities, however, means that bias can not be quantified, and may result in future predictions being being poor. We therefore extend retrospective analyses to include prediction. 

    \item When developing the design for ensembles or OMs first it is necessary to choose a base case, the main effects and interactions. A problem is that hypotheses are likely to be confounded e.g. i) is High M and low steepness realistic? ii) relative weighting of ESS and CPUE CV; and iii) dome shaped selection or senescence. If we had used the approach proposed would our conclusions have differed? Use of feedback relaxes requirement for prediction skill, ii) helps identify series for use in OEM, iii) what if OM scenarios have different "best" indices?

    %\item For models to be valid the situation being modelled must be observable and measurable. Therefore it is only possible to validate models on observations, the next step is to use model free hindcasting.
     
    %\item In order to develop robust advice frameworks by conducting Management Strategy Evaluation Operating Models should not be condition on stock assessments alone.

    %\item Hessian not inverting might be the most interesting result! i.e.  did we start from the wrong place, is there conflict or lack of information in the data, are the parameters confounded? is our structure wrong,... We need to answer these questions before we agree the ensemble.
    
    %\item The numerical tests may have different distributions and properties making using difficult to use for weighting, i.e. RMSE is influenced by a few outliers and is not comparable across series. While MASE  has the properties of scale invariance, predictable behaviour, symmetry, interpretability and asymptotic normality. MASE is also independent of the scale of the data, so can be used to compare forecasts across data sets with different scales, and behaviour is predictable as and a value of 0.5 means that a prediction is twice as good as one with a value of 1.
    
    %\item Tests have different purposes and different consequences, so equal weighting may be hard to justify. For example, if model residuals are fine, but prediction residuals fail. The consequences are  you may have overfitted, i.e. added too many parameters possibly as a result of hypothesis fishing, and the prediction residuals are telling you that your model is invalid, and so you need to start again.
    
    %\item  Bootstrapping depends on how the weights have been assigned and how the ensemble was selected.
\end{itemize}
