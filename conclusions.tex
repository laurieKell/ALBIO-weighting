\section{Conclusions}

\begin{itemize}
    \item In the stock assessment process hypotheses about states of nature are represented by alternative model structures and fixed parameters. Even with a design of 1440, as in this example, the choice of scenarios is arbitrary and the posterior distributions are often multi-modal.
    
    \item Considering alternative structures and datasets means that it difficult to select models using metrics such as AIC. Therefore retrospective analysis is commonly used to evaluate the stability of stock assessment estimates.  The use of model based quantities, however, means that bias can not be quantified, and may result in future predictions being being poor. We therefore extended retrospective analyses to include predictions. 
   
    \item The use of metrics based on prediction skill allows different data components and model to be compared in order to explore data conflicts and potential model misspecification. The accuracy and precision of predictions depend on the validity of the model, the information in the data, and how far ahead predictions are required. 
   
    \item Retrospective analysis is commonly used to evaluate the stability of stock assessment estimates of model estimates such as stock biomass and exploitation level. Stability is measured using Mohn's $\rho$ a measure of bias. 
    
    \item Shrinking estimates of stock status in the last year to the recent mean can help reduce Mohn's $\rho$. Shrinkage, in statistics, however, is used to reduce mean squared error (MSE), at the expense of bias. 
    
    \item The use of model based quantities, however, means that bias can not be quantified, and may result in future predictions being being poor. We therefore extend retrospective analyses to include prediction. 

    %\item For models to be valid the situation being modelled must be observable and measurable. Therefore it is only possible to validate models on observations, the next step is to use model free hindcasting.
     
    %\item In order to develop robust advice frameworks by conducting Management Strategy Evaluation Operating Models should not be condition on stock assessments alone.

    %\item Hessian not inverting might be the most interesting result! i.e.  did we start from the wrong place, is there conflict or lack of information in the data, are the parameters confounded? is our structure wrong,... We need to answer these questions before we agree the ensemble.
    
    %\item The numerical tests may have different distributions and properties making using difficult to use for weighting, i.e. RMSE is influenced by a few outliers and is not comparable across series. While MASE  has the properties of scale invariance, predictable behaviour, symmetry, interpretability and asymptotic normality. MASE is also independent of the scale of the data, so can be used to compare forecasts across data sets with different scales, and behaviour is predictable as and a value of 0.5 means that a prediction is twice as good as one with a value of 1.
    
    %\item Tests have different purposes and different consequences, so equal weighting may be hard to justify. For example, if model residuals are fine, but prediction residuals fail. The consequences are  you may have overfitted, i.e. added too many parameters possibly as a result of hypothesis fishing, and the prediction residuals are telling you that your model is invalid, and so you need to start again.
    
    %\item  Bootstrapping depends on how the weights have been assigned and how the ensemble was selected.
\end{itemize}
