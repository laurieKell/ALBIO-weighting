\newpage
\section*{Discussion}


When developing metrics as in any indicator, the total number should be minimised, complementary and non-redundant (Shin et al. 2010; Kershner et al. 2011). They should also be robust proxies for relevant attributes, and need to be screened using appropriate selection criteria \cite{kell2020roc}. To be effective a metric should be robust, so that it still functions despite uncertainty \parencite{radatz1990ieee, zhou1996robust}. To be robust an metric should be both reliable and stable. A metric has high reliability if despite uncertainty it provides an accurate result, and it is stable if despite random error, similar results are produced across multiple trials. 

\begin{itemize}
    \item The aim of this work was to evaluate the use of full factorial designs for representing uncertainty in stock assessment. In particular to identify the assumptions that impact the perception of stock dynamics and status, and develop tools for weighting or rejecting stock assessment scenarios.
  
    %\item The Indian Ocean Albacore MSE used Stock Synthesis to develop scenarios based on parameters that are difficult to estimate (i.e. M, steepness, and selectivity) and the relative weightings of the indices of abundance and the length data. The choice of OMs is important as the \textit{best} Management Procedure is determined by the choice of hypotheses represented by the OM. It seldom possible, however, to assign plausibility to the different scenarios, therefore the rationale for the grid design is of fundamental importance.

    %\item Need to consider the future not just the past
    
    \item The absence of retrospective patterns in model quantities such as stock biomass, is not sufficient to validate models based on model outputs, since a small RE could be achieved by a model that used no data.  Therefore conduct a hindcast.
    
    \item Can only validate models on observations and not model outputs, therefore need to conduct model free hindcasts to estimate prediction skill by comparing observations and to their model estimates, to explore bias, variability and prediction skill.
    
    \item Use the hindcast procedure to weight multi-model ensembles using simple skill-based weighting. A prediction skill score can be used to assign more weight on the better performing models as this has been found to improve forecasts \citep[e.g.][]{casanova2009weighting}. This can be done to weight estimates of current status relative to reference points, or weight operating models when conducting MSE. %Testing the approach on an actual case study first was informative as it provides the insight necessary to set up a study on synthetic data.

    %\item next step is assess prediction skill using model free hindcasting, as it allows comparisons to be made across model structures and datasets. Validation examines if a model family should be modified or extended and is complementary to model selection and hypothesis testing. In comparison model selection searches for the most suitable model within a family, whilst hypothesis testing examines if the model structure can be reduced. This is important as it allows alternative model frameworks to be compared and valiadated in a working group setting.

    %\item When conducting Management Strategy Evaluation (MSE) Operating Models (OMs) are commonly \citep{Sharma} conditioned using stock assessment models and a grid design to model structural uncertainty and data conflicts. However, there are two main characteristics of the stock dynamics i.e. the expected dynamics as represented by a production function and the nature of the times series. Therefore you could just run a limited number of scenarios that represent the different production functions an time series dynamics.

    \item \cite{fischer2020linking} showed that to develop robust management strategies the nature of the time series of the stock has to be considered. The importance of this is that trends and fluctuations in populations are determined by complex interactions between extrinsic forcing and intrinsic dynamics. For example, stochastic recruitment can induce low-frequency variability, i.e. ‘cohort resonance’, which can induce apparent trends in abundance and may be common in age-structured populations \citep[e.g.][]{bjoernstad2004trends, botsford2014cohort}. Such low-frequency fluctuations can mimic or cloak critical variation in abundance linked to environmental change, over-exploitation or other types of anthropogenic forcing. In feedback management systems the nature of the time series dynamics is likely to be more important than the equilibrium dynamics. 
    
    \item The objective of MSE is to develop a robust MP, so weighting of OM grids based on stock assessments is a waste of time. Since trends and fluctuations in populations are determined by complex interactions between extrinsic forcing and intrinsic dynamics. For example, stochastic recruitment can induce low-frequency variability, i.e. ‘cohort resonance’, which can induce apparent trends in abundance and may be common in age-structured populations \citep[e.g.][]{bjoernstad2004trends, botsford2014cohort}. Such low-frequency fluctuations can potentially mimic or cloak critical variation in abundance linked to environmental change, over-exploitation or other types of anthropogenic forcing. In feedback management systems the nature of the time series dynamics is likely to be more important than the equilibrium dynamics.

   \item Weighting give very different results wrt targets and limits, this is important, e.g. for MSC. Rather than arguing about the best weighting scheme better to develop robust MPs, that need to be tested for a range of plausible dynamics. 
   
\end{itemize} 


